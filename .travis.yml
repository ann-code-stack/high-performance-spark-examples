language: scala
sudo: false
cache:
  directories:
    - $HOME/.ivy2
    - $HOME/spark
    - $HOME/.cache/pip
    - $HOME/.sbt/launchers
scala:
   - 2.11.6
jdk:
  - oraclejdk8
addons:
  apt:
    sources:
      - ubuntu-toolchain-r-test
    packages:
      - gfortran
      - gcc
      - binutils
      - python-pip
      - python-pandas
      - python-numpy
      - gfortran
      - cmake
      - perl
r_packages:
  - Imap
before_install:
  - pip install --user codecov unittest2 nose pep8 pylint --download-cache $HOME/.pip-cache
script:
  - "export SPARK_CONF_DIR=./log4j/"
  - sbt clean coverage compile test
  - "[ -f spark] || mkdir spark && cd spark && wget http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz && cd .."
  - "tar -xf ./spark/spark-1.6.1-bin-hadoop2.6.tgz"
  - "export SPARK_HOME=`pwd`/spark-1.6.1-bin-hadoop2.6"
  - "export PYTHONPATH=$SPARK_HOME/python:`ls -1 $SPARK_HOME/python/lib/py4j-*-src.zip`:$PYTHONPATH"
  - "nosetests --with-doctest --doctest-options=+ELLIPSIS --logging-level=INFO --detailed-errors --verbosity=2 --with-coverage --cover-html-dir=./htmlcov"
after_success:
# For now no coverage report
  - codecov